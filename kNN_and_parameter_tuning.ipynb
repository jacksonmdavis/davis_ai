{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jackson Davis k-Nearest-Neighbors, Cross-validation, and Parameter Tuning demo\n",
    "\n",
    "This notebook serves as a demonstration of some python code I wrote to implement k-Nearest-Neighbors and parameter tuning.  There was a significant focus on documentation and unit testing here, as well as getting a good handle on higher-order functions, generators, and other basic python concepts.  The code is not optimized for speed, but rather for readability and ease of use, and uses only python primitive data structures (e.g. no arrays or dataframes) as well as a functional, non-OOP programming style.\n",
    "\n",
    "The dataset used as a demonstration here is a popular Kaggle dataset for machine learning, the [Red Wine Quality dataset](https://www.kaggle.com/datasets/uciml/red-wine-quality-cortez-et-al-2009).  The dataset is included in the repo as `Datasets/winequality-red.csv`.\n",
    "\n",
    "For more information on myself or for contact, see https://www.sevenhillsmt.com/jackson-portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List, Dict, Tuple, Callable\n",
    "from copy import deepcopy\n",
    "from math import sqrt\n",
    "import numpy as np # Used for simple functions only, e.g. np.mean()\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After imports, the first thing we do is load up the dataset.  Because each dataset is structured somewhat differently, this function is not documented for reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name: str) -> tuple[list, list]:\n",
    "    with open(file_name, \"r\") as f:\n",
    "        header = f.readline().rstrip().split(\",\")\n",
    "        data = []\n",
    "        for line in f:\n",
    "            datum = [float(value) for value in line.rstrip().split(\",\")]\n",
    "            data.append(datum)\n",
    "        random.shuffle(data)\n",
    "    return data, header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(3)\n",
    "data = load_data(\"Datasets\\winequality-red.csv\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8.0, 0.42, 0.17, 2.0, 0.073, 6.0, 18.0, 0.9972, 3.29, 0.61, 9.2, 6.0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1599"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the data is already loaded into a list of observations, with the target value as the final element of each observation, which is compatible with the code below.  The target value is a quality score from 0-10, and we will be using this as both a regression problem and a classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardization\n",
    "\n",
    "First a couple of simple functions to further process the data such that it is standardized.  This is a common preprocessing step for k-Nearest-Neighbors, as the algorithm is sensitive to the scale of the data.  We will be using the z-score standardization method, which is to subtract the mean and divide by the standard deviation for each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"z_score_standardize\"></a>\n",
    "## z_score_standardize\n",
    "\n",
    "`z_score_standardize` is a function that takes a dataset and returns a new dataset where each feature has been standardized to have a mean of 0 and a standard deviation of 1.  This is useful for improving the performance of some machine learning algorithms, such as K Nearest Neighbors, that are sensitive to the scale of the features.  Standardizing the features ensures that each feature is treated equally by the algorithm.\n",
    "\n",
    "* **data** `list[list]`: a list of observations, each of which is a list of values for each feature.  **There should be no target values in this dataset.**\n",
    "\n",
    "**return** `list[list]`: the same observations, but with each feature standardized to have a mean of 0 and a standard deviation of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score_standardize(data: list[list]) -> list[list]:\n",
    "    data = deepcopy(data)\n",
    "    for i in range(len(data[0])):\n",
    "        column = [row[i] for row in data]\n",
    "        mean = np.mean(column)\n",
    "        std = np.std(column)\n",
    "        for row in data:\n",
    "            row[i] = (row[i] - mean) / std\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [[0,0,0,0], [1,1,1,2], [2,2,2,6]]\n",
    "test_z_score = z_score_standardize(test_data)\n",
    "assert np.allclose(np.mean(test_z_score, axis=0), [0,0,0,0])\n",
    "assert np.allclose(np.std(test_z_score, axis=0), [1,1,1,1])\n",
    "\n",
    "test_data = np.random.rand(100, 4)\n",
    "test_z_score = z_score_standardize(test_data)\n",
    "assert len(test_z_score) == 100\n",
    "assert np.allclose(np.mean(test_z_score, axis=0), [0,0,0,0])\n",
    "assert np.allclose(np.std(test_z_score, axis=0), [1,1,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"process_data_x_only\"></a>\n",
    "## process_data_x_only\n",
    "\n",
    "`process_data_x_only` is a helper function that takes data in which the target value is the last value in each observation, separates the observation data from the target values, and runs a given process on the observations (without the target values) before recombining.  It could be taken a step further and include a `split` and `recombine` function, but the risk of drowning in documentation may be increased.\n",
    "\n",
    "* **data** `list[list]`: a list of observations, each of which is a list of values for each feature.  **The target value is the last item in each observation.**\n",
    "* **process** `callable`: a function that takes a dataset and returns a new, processed dataset\n",
    "\n",
    "**return** `list[list]`: the same observations, but with a process run on the non-target portion of each observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_x_only(data: list[list], process: callable) -> list[list]:\n",
    "    data = deepcopy(data)\n",
    "    \n",
    "    data_x = [row[:-1] for row in data]\n",
    "    data_y = [[row[-1]] for row in data]\n",
    "    data_x = process(data_x)\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        data[i] = data_x[i] + data_y[i]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [[1,1,1,1], [1,1,1,1], [1,1,1,1]]\n",
    "test_processed = process_data_x_only(test_data, lambda x: [[0,0,0], [0,0,0], [0,0,0]])\n",
    "assert test_processed == [[0,0,0,1], [0,0,0,1], [0,0,0,1]]\n",
    "test_processed = process_data_x_only(test_data, lambda x: [[1,0,0], [0,1,0], [0,0,1]])\n",
    "assert test_processed == [[1,0,0,1], [0,1,0,1], [0,0,1,1]]\n",
    "test_processed = process_data_x_only(test_data, lambda x: [[1,1,0], [0,1,1], [1,0,1]])\n",
    "assert test_processed == [[1,1,0,1], [0,1,1,1], [1,0,1,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.18364134481694747, -0.6023367688141634, -0.5185144220985399, -0.38227061366211557, -0.30746785242725816, -0.9443463560547672, -0.8656761636507785, 0.24026624565698693, -0.13679826698989317, -0.2841411376234579, -1.1479806048222734, 6.0]\n"
     ]
    }
   ],
   "source": [
    "data = process_data_x_only(data, z_score_standardize)\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kNN algorithm\n",
    "\n",
    "The functions below are the building blocks of a k-Nearest-Neighbors (kNN) algorithm, a simple and very common classificiation/regression algorithm.  The algorithm is very simple: given a set of observations with known target values, and a new observation with an unknown target value, find the k most similar observations in the dataset, and use their target values to predict the target value of the new observation.  The similarity metric used here is the Euclidean distance between the observations, but other metrics could be used as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"euclidean_distance\"></a>\n",
    "## euclidean_distance\n",
    "\n",
    "`euclidean_distance` calculates the Euclidean distance between two points in n-dimensional space according to the following formula:\n",
    "$$d(x_1,x_2) = \\sqrt{\\sum^n_{i=1}(x_{1i} - x_{2i})^2}$$\n",
    "In the [knn](#knn) algorithm, where we are searching for `K Nearest Neighbors`, we use this function as the metric that defines `nearest`.  For this use case, we do not need to find the *actual* distance, just the *relative* distance between two points, so we can omit the square root operation when calling this as part of K Nearest Neighbors to make the function a bit faster.\n",
    "\n",
    "**Used by**: [kNN](#kNN)\n",
    "\n",
    "* **x1** `list`: a list of values representing a point in n-dimensional space\n",
    "* **x2** `list`: a list of values representing a second point in n-dimensional space\n",
    "* **do_sqrt** `bool`: whether or not to take the square root of the squared distance when calculating\n",
    "\n",
    "**return** `float`: the Euclidean distance between the two points (possibly squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(x1: list, x2: list, do_sqrt: bool = True) -> float:\n",
    "    distance_squared = sum((x1_i - x2_i) ** 2 for x1_i, x2_i in zip(x1, x2))\n",
    "    if do_sqrt:\n",
    "        return sqrt(distance_squared)\n",
    "    else:\n",
    "        return distance_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert euclidean_distance([0, 0], [1, 1]) == sqrt(2)\n",
    "assert euclidean_distance([0, 0], [1, 1], do_sqrt=False) == 2\n",
    "assert euclidean_distance([0, 0], [0, 0]) == 0\n",
    "assert euclidean_distance([-1, 0], [1, 0]) == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"knn_real_regression\"></a>\n",
    "## knn_real_regression\n",
    "\n",
    "`knn_real_regression` is a function that processes the list of *real \"y\" values only* for the K points found via the [knn](#knn) algorithm and simply returns the mean of these values.  It is for the real-valued regression use case of KNN.  It probably could just be called \"mean\", but it serves as a semantic reminder of its function and a complement to the categorical use case.\n",
    "\n",
    "**Used by**: [kNN](#kNN)\n",
    "\n",
    "* **nearest** `list[float]`: a list of target values for the K Nearest Neighbors\n",
    "\n",
    "**return** `float`: the mean of the target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_real_regression(nearest: list[float]) -> float:\n",
    "    return np.mean(nearest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [1,2,3]\n",
    "assert knn_real_regression(test_data) == 2\n",
    "test_data = [1,2,3,4]\n",
    "assert knn_real_regression(test_data) == 2.5\n",
    "test_data = [-1,-2,0,1,2]\n",
    "assert knn_real_regression(test_data) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"knn_cat_classification\"></a>\n",
    "## knn_cat_classification\n",
    "\n",
    "`knn_cat_classification` is a function that processes the list of *categorical \"y\" values only* for the K points found via the [knn](#knn) algorithm and returns the most frequent category value in the list.  It is for the categorical classification use case of KNN.  It could probably just be called \"find_most_frequent\", but it serves as a semantic reminder of its function and a complement to the regression use case.\n",
    "\n",
    "**Used by**: [kNN](#kNN)\n",
    "\n",
    "* **nearest** `list[str]`: a list of target classes for the K Nearest Neighbors\n",
    "\n",
    "**return** `str`: the most frequent class in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_cat_classification(nearest: list[str]) -> str:\n",
    "    classes, counts = np.unique(nearest, return_counts=True)\n",
    "    return classes[np.argmax(counts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [1,2,2]\n",
    "assert knn_cat_classification(test_data) == 2\n",
    "test_data = [1,2,3,3]\n",
    "assert knn_cat_classification(test_data) == 3\n",
    "test_data = ['blue','blue','green','red']\n",
    "assert knn_cat_classification(test_data) == 'blue'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"knn\"></a>\n",
    "## knn\n",
    "\n",
    "`knn` is an implementation of the \"K Nearest Neighbors\" algorithm for classification and regression.  It takes a \"knowledge base\" dataset of observations and their target values, a new observation, and a value for K, and returns the predicted target value for the new observation.  The algorithm works by finding the K observations *nearest* to the new observation in the knowledge base and then using the target values of those K observations to predict the target value of the new observation.  For regression, the predicted target value is the mean of the K target values.  For classification, the predicted target value is the most frequent target value in the K target values.\n",
    "\n",
    "**Used by**: \n",
    "**Uses**: [euclidean_distance](#euclidean_distance), [knn_real_regression](#knn_real_regression), [knn_cat_classification](#knn_cat_classification)\n",
    "\n",
    "* **dataset** `list[list]`: a list of observations, each of which is a list of values for each dimension.  **The target value is the last value in each dataset observation.**\n",
    "* **query_point** `list`: a list representing a single observation with no target value.  **len(query_point) == len(dataset[0]) - 1 should be True**\n",
    "* **processing** `callable`: a function that processes the list of target values for the K Nearest Neighbors and returns a single value.  For regression, this is [knn_real_regression](#knn_real_regression).  For classification, this is [knn_cat_classification](#knn_cat_classification).\n",
    "* **k** `int`: the number of nearest neighbors to find and use for prediction\n",
    "\n",
    "**return** `float|str`: the predicted target value for the new observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(dataset: list, query_point: list, processing: callable, k: int) -> float:\n",
    "    distances = []\n",
    "    for datapoint in dataset:\n",
    "        distances.append((euclidean_distance(datapoint[:-1], query_point, do_sqrt=False), datapoint))\n",
    "    distances.sort(key=lambda x: x[0])\n",
    "    nearest_values = [item[1][-1] for item in distances[:k]]\n",
    "    return processing(nearest_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [[0,0,0,'blue'], [1,1,1,'blue'], [2,2,2,'red']]\n",
    "test_query = [1,1,1]\n",
    "assert knn(test_data, test_query, knn_cat_classification, 2) == 'blue'\n",
    "\n",
    "test_data = [[0,0,0,1], [1,1,1,1], [2,2,2,6], [3,3,3,0]]\n",
    "test_query = [9,9,9]\n",
    "assert knn(test_data, test_query, knn_real_regression, 4) == 2\n",
    "assert knn(test_data, test_query, knn_cat_classification, 4) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"make_knn_func\"></a>\n",
    "## make_knn_func\n",
    "\n",
    "`make_knn_func` is wrapper function that takes a dataset and a value for K and returns a K Nearest Neighbors ([knn](#knn)) function that can be used to make predictions for new observations.  This is useful for the [k_fold_evaluate](#k_fold_evaluate) function because it allows us to pass our KNN function in as a general 'classifier' function (with appropriate arguments) to be evaluated.\n",
    "\n",
    "**Uses**: [knn](#knn)\n",
    "**Used by**: [k_fold_evaluate](#k_fold_evaluate)\n",
    "\n",
    "* **processing** `callable`: the method for processing the list of target values to return a single value.  For regression, this is [knn_real_regression](#knn_real_regression).  For classification, this is [knn_cat_classification](#knn_cat_classification).\n",
    "\n",
    "**return** `callable`: a K Nearest Neighbors function that can be used to make predictions for new observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_knn_func(processing: callable, k: int):\n",
    "    def knn_func(dataset: list, query_point: list):\n",
    "        return knn(dataset, query_point, processing, k)\n",
    "    return knn_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same tests as before\n",
    "test_data = [[0,0,0,'blue'], [1,1,1,'blue'], [2,2,2,'red']]\n",
    "test_query = [1,1,1]\n",
    "knn_func = make_knn_func(knn_cat_classification, 2)\n",
    "assert knn_func(test_data, test_query) == 'blue'\n",
    "\n",
    "test_data = [[0,0,0,1], [1,1,1,1], [2,2,2,6], [3,3,3,0]]\n",
    "test_query = [9,9,9]\n",
    "knn_func = make_knn_func(knn_real_regression, 4)\n",
    "assert knn_func(test_data, test_query) == 2\n",
    "knn_func = make_knn_func(knn_cat_classification, 4)\n",
    "assert knn_func(test_data, test_query) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation\n",
    "\n",
    "Now that kNN has been built, below is a simple implementation of k-fold cross-validation (yes, multiple uses of the letter 'k' is confusing), as well as some other simple performance validation tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"k_fold_iterator\"></a>\n",
    "## k_fold_iterator\n",
    "\n",
    "`k_fold_iterator` is a helper function that takes a dataset and a value for K and returns a generator that yields a list of K train/test folds for K-fold evaluation.  Each fold is a list of two lists: the first list is the training set and the second list is the test set.  **This function does not take specific target value location into account but assumes the target value is contained within each observation.**\n",
    "\n",
    "**Used by**: [k_fold_evaluate](#k_fold_evaluate)\n",
    "\n",
    "* **data** `list`: a list of observations\n",
    "* **k** `int`: the number of folds to create\n",
    "* **num_tests** `int`: length of the list of folds to return, if other than K.  This is useful for creating a single or small number of train/test folds, e.g. for parameter tuning.\n",
    "\n",
    "**yield** `Tuple[list[list], list[list]]`: the train and test sets for each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_iterator(data:list, k: int = 10, num_tests: int = None):\n",
    "    if num_tests is None:\n",
    "        num_tests = k\n",
    "    random.shuffle(data)\n",
    "\n",
    "    for i in range(num_tests):\n",
    "        test = data[i*len(data)//k : (i+1)*len(data)//k]\n",
    "        train = data[:i*len(data)//k] + data[(i+1)*len(data)//k:]\n",
    "        yield train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_test_data = [[0,0,0,0], [1,1,1,2], [2,2,2,6]]\n",
    "answers = [4,3,1]\n",
    "\n",
    "for training, test in k_fold_iterator(fold_test_data, 3):\n",
    "    assert knn(training, test[0][:-1], knn_real_regression, 2) in answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"mean_squared_error\"></a>\n",
    "## mean_squared_error\n",
    "\n",
    "`mean_squared_error` is a function that calculates the mean squared error between two lists of real values.  It is used to evaluate the performance of a regression model by comparing the predicted target values to the actual target values.  The formula for MSE is:\n",
    "$$MSE = \\frac{1}{n}\\sum^n_i (y_i - \\hat{y}_i)^2$$\n",
    "where $y_i$ is the actual target value and $\\hat{y}_i$ is the predicted target value.\n",
    "\n",
    "**Used by**: [k_fold_evaluate](#k_fold_evaluate)\n",
    "\n",
    "* **predicted** `list[float]`: a list of predicted target values\n",
    "* **actual** `list[float]`: a list of actual target values\n",
    "\n",
    "**return** `float`: the mean squared error between the lists of predicted and actual target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(predicted: list, actual: list) -> float:\n",
    "    return np.mean([(predicted_value - actual_value) ** 2 for predicted_value, actual_value in zip(predicted, actual)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert mean_squared_error([1,2,3], [1,2,3]) == 0\n",
    "assert mean_squared_error([1,2,3,4], [3,2,1,4]) == 2\n",
    "assert mean_squared_error([3,-1,4], [3,2,4]) == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"simple_accuracy\"></a>\n",
    "## simple_accuracy\n",
    "\n",
    "`simple_accuracy` is a function that compares a list of predicted target values to a list of actual target values and returns the percentage of correct predictions.  It is used to evaluate the performance of a classification model, and is best used when classes are relatively balanced within the dataset.\n",
    "\n",
    "**Used by**: [k_fold_evaluate](#k_fold_evaluate)\n",
    "\n",
    "* **predicted** `list[float]`: a list of predicted target values\n",
    "* **actual** `list[float]`: a list of actual target values\n",
    "\n",
    "**return** `float`: the percentage of correct predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_accuracy(predicted: list, actual: list) -> float:\n",
    "    return np.mean([predicted_value == actual_value for predicted_value, actual_value in zip(predicted, actual)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert simple_accuracy([1,2,3], [1,2,3]) == 1\n",
    "assert simple_accuracy([1,2,3,4], [3,2,1,4]) == 0.5\n",
    "assert simple_accuracy([3,-1,4], [3,2,4]) == 2/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"results_printer\"></a>\n",
    "## results_printer\n",
    "\n",
    "`results_printer` is a printing function that accepts a list of results from [k_fold_evaluate](#k_fold_evaluate) and prints the results, split into 'training' and 'testing' evaluations, in a nicely formatted table.  It accepts a dictionary in the form `{'train': [], 'test': []}` where the lists are the results from each fold.  It also prints the mean and std of the results.\n",
    "\n",
    "**Used by**: [k_fold_evaluate](#k_fold_evaluate)\n",
    "\n",
    "* **results** `dict`: a dictionary of results from [k_fold_evaluate](#k_fold_evaluate)\n",
    "* **verbose** `bool`: whether or not to print the individual results for each fold\n",
    "\n",
    "**no return (This function has side effects, i.e. the printing)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_printer(results: dict, verbose: bool = True):\n",
    "    if verbose:\n",
    "        print(\"Fold\\t  Train\\t\\t Test\")\n",
    "        for i in range(len(results[\"train\"])): # same length as test\n",
    "            print(f\"{i+1}:\\t  {format(results['train'][i], '.4f')} \\t {format(results['test'][i], '.4f')}\")\n",
    "        print(\"-----\")\n",
    "    print(f\"avg\\t  {format(np.mean(results['train']), '.4f')} \\t {format(np.mean(results['test']), '.4f')}\")\n",
    "    print(f\"std\\t  {format(np.std(results['train']), '.4f')} \\t {format(np.std(results['test']), '.4f')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"classify\"></a>\n",
    "## classify\n",
    "\n",
    "`classify` is an intermediary function that takes a set of training data, a set of test data, and a classifier function and returns the results of the classifier function on the training and test data.  It is used by [k_fold_evaluate](#k_fold_evaluate) to evaluate the performance of a classifier function on a set of folds.\n",
    "\n",
    "**Used by**: [k_fold_evaluate](#k_fold_evaluate)\n",
    "\n",
    "* **train_data** `list[list]`: a list of training observations.  **The target value is the last value in each observation.**\n",
    "* **test_data** `list[list]`: a list of testing observations.  **Test data should also have the target value as the last element in each observation.  If none is available, I guess you have to put in a dummy value or everything just falls right apart.**\n",
    "* **classifier** `callable`: a classifier function that takes a dataset and an observation and returns a predicted target value\n",
    "\n",
    "**return** `list`: a list of predicted target values for the observations in the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(train_data: list[list], test_data: list[list], classifier) -> list:\n",
    "    predicted = []\n",
    "    for test_point in test_data:\n",
    "        predicted.append(classifier(train_data, test_point[:-1]))\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"baseline_regression\"></a>\n",
    "## baseline_regression\n",
    "\n",
    "`baseline_regression` is a simple 'regression' function that takes a training dataset and returns the mean of the target values in the dataset as the prediction for the target value of each observation in the test set.  It is used as the baseline model for comparison with the other regression models.\n",
    "\n",
    "**Used by**: [k_fold_evaluate](#k_fold_evaluate)\n",
    "\n",
    "* **train_data** `list[list]`: a list of training observations.  **The target value is the last value in each observation.**\n",
    "* **test_point** `list`: a query point to be 'classified' (regressed??).  **This parameter is unused, but included for compatibility**\n",
    "\n",
    "**return** `list`: a list of predicted target values for the observations in the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_regression(train_data: list[list], test_point: list) -> float:\n",
    "    return np.mean([train_point[-1] for train_point in train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [[0,0,0,1], [1,1,1,1], [2,2,2,2], [3,3,3,2]]\n",
    "test_query = [1,2,3,4]\n",
    "assert baseline_regression(test_data, test_query) == 1.5\n",
    "test_data = [[9,1], [9,2], [9,3], [9,4]]\n",
    "test_query = [12,16]\n",
    "assert baseline_regression(test_data, test_query) == 2.5\n",
    "test_data = [[1,2], [2,2], [3,4], [4,4]]\n",
    "test_query = ['gibberish']\n",
    "assert baseline_regression(test_data, test_query) == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"baseline_classification\"></a>\n",
    "## baseline_classification\n",
    "\n",
    "`baseline_classification` is a simple 'classification' function that takes a training dataset and returns the most frequent target value in the dataset as the prediction for the target value of each observation in the test set.  It is used as the baseline model for comparison with the other classification models.\n",
    "\n",
    "**Used by**: [k_fold_evaluate](#k_fold_evaluate)\n",
    "\n",
    "* **train_data** `list[list]`: a list of training observations.  **The target value is the last value in each observation.**\n",
    "* **test_point** `list`: a query point to be 'classified'.  **This parameter is unused, but included for compatibility**\n",
    "\n",
    "**return** `list`: a list of predicted target values for the observations in the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_classification(train_data: list[list], test_point: list) -> str:\n",
    "    classes, counts = np.unique([train_point[-1] for train_point in train_data], return_counts=True)\n",
    "    return classes[np.argmax(counts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [[0,0,0,1], [1,1,1,1], [2,2,2,2], [3,3,3,3]]\n",
    "test_query = [1,2,3,4]\n",
    "assert baseline_classification(test_data, test_query) == 1\n",
    "test_data = [[9,'cat'], [6,'cat'], [12,'dog']]\n",
    "test_query = [12,16]\n",
    "assert baseline_classification(test_data, test_query) == 'cat'\n",
    "test_data = [[1,2], [2,3], [3,4], [4,4]]\n",
    "test_query = ['gibberish']\n",
    "assert baseline_classification(test_data, test_query) == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"k_fold_evaluate\"></a>\n",
    "## k_fold_evaluate\n",
    "\n",
    "`k_fold_evaluate` is a pipeline function for evaluating the performance of classifiers on a dataset using k-fold cross validation.  It takes a set of data, splits it into k train-test folds for evaluation, and passes the data into the given classifier function. It then evaluates the results with a given evaluation metric.  The function also optionally prints the results of the evaluation using [results_printer](#results_printer).\n",
    "\n",
    "**Uses**: [iterate_folds](#iterate_folds), [classify](#classify), [results_printer](#results_printer)\n",
    "\n",
    "* **data** `list`: the dataset to be split into folds and evaluated\n",
    "* **k** `int`: the number of folds to split the data into\n",
    "* **classifier** `callable`: a classifier function that takes a dataset and an observation and returns a predicted target value\n",
    "* **evaluator** `callable`: an evaluation metric function that takes a list of predicted target values and a list of actual target values and returns a single value representing a generalization error metric\n",
    "* **num_tests** `int`: the number of times to run the evaluation if you don't want to loop through all the folds (because it takes a long time).  Default None runs the evaluation on all the folds.\n",
    "* **print_results** `bool`: whether or not to print the results of the evaluation\n",
    "* **verbose** `bool`: whether or not to print the individual results for each fold\n",
    "\n",
    "\n",
    "**return** `dict`: a dictionary of results from the evaluation in the form `{'avg_train': float, 'std_train': float, 'avg_test': float, 'std_test': float}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_evaluate(data: list, evaluator: callable, classifier: callable, *, k_folds: int = 10, num_tests: int = None, print_results:bool = True, verbose:bool = True) -> float:\n",
    "    results = {'train': [], 'test': []}\n",
    "    for training, test in k_fold_iterator(data, k=k_folds, num_tests=num_tests):\n",
    "        results['train'].append(evaluator(classify(training, training, classifier), [train_point[-1] for train_point in training]))\n",
    "        results['test'].append(evaluator(classify(training, test, classifier), [test_point[-1] for test_point in test]))\n",
    "        \n",
    "    if print_results:\n",
    "        results_printer(results, verbose)\n",
    "    \n",
    "    returns = {'avg_train': np.mean(results['train']), 'std_train': np.std(results['train']),\n",
    "               'avg_test': np.mean(results['test']), 'std_test': np.std(results['test'])}\n",
    "\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "For our example, we want to choose the value of k that provides us with the most accuracy while still generalizing well to new data.  We will use the `k_fold_evaluate` function and some quick throwaway code to evaluate the performance of our kNN algorithm on a range of k values, and then choose the k value that provides the best performance.  This example uses a smaller dataset for efficiency, as the runtime of kNN is not exactly fast, and the larger the dataset (and the larger value of k), the longer it takes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best k: 23 with average MSE: 0.4115146502835539\n"
     ]
    }
   ],
   "source": [
    "ks = list(range(1, 41, 2))\n",
    "train_ks = []\n",
    "test_ks = []\n",
    "average_results = []\n",
    "for k in ks:\n",
    "    test_result = k_fold_evaluate(data[:800], mean_squared_error, make_knn_func(knn_real_regression, k), print_results=False, verbose=False)\n",
    "    train_ks.append(test_result['avg_train'])\n",
    "    test_ks.append(test_result['avg_test'])\n",
    "    average_results.append((test_result['avg_test'], k))\n",
    "average_results.sort(key=lambda x: x[0])\n",
    "best_result = average_results[0]\n",
    "print(f\"Best k: {best_result[1]} with average MSE: {best_result[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(ks, train_ks, 'b', ks, test_ks, 'r')\n",
    "plt.legend(['Train', 'Test'])\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('MSE')\n",
    "plt.gca().invert_xaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Finally, having tuned our kNN algorithm, we can evaluate its performance with cross-fold validation.  First we make the kNN functions, both classification and regression, with k=17.  Then we compare the performance of the kNN algorithm to the baseline models, and print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_cat = make_knn_func(knn_cat_classification, 17)\n",
    "knn_reg = make_knn_func(knn_real_regression, 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline classification:\n",
      "Fold\t  Train\t\t Test\n",
      "1:\t  0.4299 \t 0.3899\n",
      "2:\t  0.4295 \t 0.3937\n",
      "3:\t  0.4281 \t 0.4062\n",
      "4:\t  0.4218 \t 0.4625\n",
      "5:\t  0.4274 \t 0.4125\n",
      "6:\t  0.4218 \t 0.4625\n",
      "7:\t  0.4274 \t 0.4125\n",
      "8:\t  0.4225 \t 0.4562\n",
      "9:\t  0.4267 \t 0.4188\n",
      "10:\t  0.4239 \t 0.4437\n",
      "-----\n",
      "avg\t  0.4259 \t 0.4259\n",
      "std\t  0.0029 \t 0.0265\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline classification:\")\n",
    "test_result = k_fold_evaluate(data, simple_accuracy, baseline_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN classification:\n",
      "Fold\t  Train\t\t Test\n",
      "1:\t  0.6438 \t 0.5409\n",
      "2:\t  0.6567 \t 0.4562\n",
      "3:\t  0.6268 \t 0.6937\n",
      "4:\t  0.6379 \t 0.6250\n",
      "5:\t  0.6421 \t 0.5875\n",
      "6:\t  0.6407 \t 0.6312\n",
      "7:\t  0.6324 \t 0.6188\n",
      "8:\t  0.6379 \t 0.6125\n",
      "9:\t  0.6567 \t 0.5625\n",
      "10:\t  0.6442 \t 0.5375\n",
      "-----\n",
      "avg\t  0.6419 \t 0.5866\n",
      "std\t  0.0089 \t 0.0622\n"
     ]
    }
   ],
   "source": [
    "print(\"kNN classification:\")\n",
    "test_result = k_fold_evaluate(data, simple_accuracy, knn_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline regression:\n",
      "Fold\t  Train\t\t Test\n",
      "1:\t  0.6416 \t 0.7438\n",
      "2:\t  0.6573 \t 0.6023\n",
      "3:\t  0.6669 \t 0.5152\n",
      "4:\t  0.6426 \t 0.7346\n",
      "5:\t  0.6549 \t 0.6238\n",
      "6:\t  0.6430 \t 0.7306\n",
      "7:\t  0.6561 \t 0.6147\n",
      "8:\t  0.6509 \t 0.6596\n",
      "9:\t  0.6664 \t 0.5211\n",
      "10:\t  0.6374 \t 0.7811\n",
      "-----\n",
      "avg\t  0.6517 \t 0.6527\n",
      "std\t  0.0099 \t 0.0887\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline regression:\")\n",
    "test_result = k_fold_evaluate(data, mean_squared_error, baseline_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN regression:\n",
      "Fold\t  Train\t\t Test\n",
      "1:\t  0.3798 \t 0.4748\n",
      "2:\t  0.3870 \t 0.3993\n",
      "3:\t  0.3837 \t 0.4438\n",
      "4:\t  0.3872 \t 0.3983\n",
      "5:\t  0.3867 \t 0.4299\n",
      "6:\t  0.3841 \t 0.4270\n",
      "7:\t  0.3825 \t 0.4546\n",
      "8:\t  0.3854 \t 0.4293\n",
      "9:\t  0.3893 \t 0.3652\n",
      "10:\t  0.3790 \t 0.4974\n",
      "-----\n",
      "avg\t  0.3845 \t 0.4320\n",
      "std\t  0.0032 \t 0.0367\n"
     ]
    }
   ],
   "source": [
    "print(\"kNN regression:\")\n",
    "test_result = k_fold_evaluate(data, mean_squared_error, knn_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the kNN algorithm performs better than the baseline models, with a mean accuracy of 0.59 compared to 0.43 for the baseline classification model, and a mean squared error of 0.43 compared to 0.65 for the baseline regression model.  This is a good sign that our kNN algorithm is working well, and that our parameter tuning was successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final check, it's always nice to compare to an established library.  Below, we use the `KNeighborsRegressor` and `KNeighborsClassifier` classes from the `sklearn` library to compare the performance of our kNN algorithm to the performance of the `sklearn` implementation.  We can see that the performance of our kNN algorithm is very similar to the performance of the `sklearn` implementation, which is a good sign that our implementation is working well.  The sklearn library is, of course, much faster and more optimized than our implementation, and running it only takes a few lines of code, but this is how we learn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = [row[:-1] for row in data]\n",
    "data_y = [row[-1] for row in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn kNN regression: 0.432907528336412\n"
     ]
    }
   ],
   "source": [
    "model = sklearn.neighbors.KNeighborsRegressor(n_neighbors=17)\n",
    "model.fit(data_x[:1200], data_y[:1200])\n",
    "predicted = model.predict(data_x[1200:])\n",
    "print(f\"Sklearn kNN regression: {mean_squared_error(predicted, data_y[1200:])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn kNN classification: 0.5789473684210527\n"
     ]
    }
   ],
   "source": [
    "model = sklearn.neighbors.KNeighborsClassifier(n_neighbors=17)\n",
    "model.fit(data_x[:1200], data_y[:1200])\n",
    "predicted = model.predict(data_x[1200:])\n",
    "print(f\"Sklearn kNN classification: {simple_accuracy(predicted, data_y[1200:])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "en605645",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "117px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
